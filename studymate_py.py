# -*- coding: utf-8 -*-
"""studymate.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FbpTX0CmmOsIvnCpYzxYh4p6ouxUwctu
"""

!pip install transformers sentence-transformers torch faiss-cpu PyMuPDF streamlit pyngrok
!pip install accelerate datasets evaluate

import os
import time
import torch
import fitz  # PyMuPDF
import numpy as np
import pandas as pd
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import faiss
from google.colab import files
import streamlit as st

class PDFProcessor:
    def __init__(self):
        pass

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF file"""
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text

    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

class VectorStore:
    def __init__(self, embedding_model_name: str = "all-MiniLM-L6-v2"):
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.index = None
        self.texts = []

    def add_texts(self, texts: List[str]):
        """Add texts to vector store"""
        self.texts.extend(texts)
        embeddings = self.embedding_model.encode(texts)

        if self.index is None:
            # Initialize FAISS index
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner product for similarity

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))

    def search(self, query: str, k: int = 5) -> List[Dict]:
        """Search for similar texts"""
        if self.index is None or len(self.texts) == 0:
            return []

        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.texts):
                results.append({
                    'text': self.texts[idx],
                    'score': float(score),
                    'index': int(idx)
                })

        return results

class ConversationMemory:
    def __init__(self, max_history: int = 10):
        self.exchanges = []
        self.max_history = max_history
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def add_exchange(self, question: str, answer: str, metadata: Dict = None):
        """Add Q&A exchange to memory"""
        exchange = {
            'question': question,
            'answer': answer,
            'timestamp': time.time(),
            'metadata': metadata or {}
        }

        self.exchanges.append(exchange)

        # Keep only recent exchanges
        if len(self.exchanges) > self.max_history:
            self.exchanges = self.exchanges[-self.max_history:]

    def get_recent_context(self, n: int = 3) -> List[Dict]:
        """Get recent conversation context"""
        return self.exchanges[-n:] if self.exchanges else []

    def get_relevant_context(self, question: str, n: int = 2) -> List[Dict]:
        """Get contextually relevant previous exchanges"""
        if not self.exchanges:
            return []

        # Create embeddings for question and previous questions
        question_embedding = self.embedding_model.encode([question])
        previous_questions = [ex['question'] for ex in self.exchanges]

        if not previous_questions:
            return []

        previous_embeddings = self.embedding_model.encode(previous_questions)

        # Calculate similarities
        similarities = np.dot(question_embedding, previous_embeddings.T)[0]

        # Get top similar exchanges
        top_indices = np.argsort(similarities)[-n:][::-1]

        return [self.exchanges[i] for i in top_indices if similarities[i] > 0.3]

class StudyMate:
    def __init__(self):
        self.pdf_processor = PDFProcessor()
        self.vector_store = VectorStore()
        self.memory = ConversationMemory()

        # Initialize QA pipeline with HuggingFace
        self.qa_pipeline = pipeline(
            "question-answering",
            model="distilbert-base-cased-distilled-squad",
            tokenizer="distilbert-base-cased-distilled-squad"
        )

        # Alternative: Use a better model
        # self.qa_pipeline = pipeline(
        #     "question-answering",
        #     model="deepset/roberta-base-squad2"
        # )

    def upload_and_process_pdf(self, pdf_path: str):
        """Process uploaded PDF"""
        print(f"Processing PDF: {pdf_path}")

        # Extract text
        text = self.pdf_processor.extract_text_from_pdf(pdf_path)
        print(f"Extracted {len(text)} characters")

        # Chunk text
        chunks = self.pdf_processor.chunk_text(text)
        print(f"Created {len(chunks)} chunks")

        # Add to vector store
        self.vector_store.add_texts(chunks)
        print("‚úÖ PDF processed and indexed!")

        return len(chunks)

    def ask_question(self, question: str) -> Dict[str, Any]:
        """Answer question based on uploaded documents"""
        start_time = time.time()

        # Get relevant context from documents
        search_results = self.vector_store.search(question, k=3)

        if not search_results:
            return {
                'answer': "No relevant information found. Please upload a PDF document first.",
                'confidence': 0.0,
                'sources': [],
                'query_time': time.time() - start_time
            }

        # Combine context from top search results
        context = " ".join([result['text'] for result in search_results])

        # Get conversation context
        recent_context = self.memory.get_recent_context(2)
        relevant_context = self.memory.get_relevant_context(question, 2)

        # Enhance context with conversation history
        conversation_context = ""
        if recent_context or relevant_context:
            conversation_context = self._format_conversation_context(recent_context, relevant_context)
            context = f"{context}\n\nPrevious conversation:\n{conversation_context}"

        # Truncate context if too long (BERT has token limits)
        max_length = 512 - len(question.split()) - 50  # Reserve space for question and answer
        context_words = context.split()
        if len(context_words) > max_length:
            context = " ".join(context_words[:max_length])

        try:
            # Get answer using HuggingFace pipeline
            result = self.qa_pipeline(question=question, context=context)

            response = {
                'answer': result['answer'],
                'confidence': result['score'],
                'sources': search_results[:2],
                'query_time': time.time() - start_time,
                'context_used': len(context.split()) if context else 0
            }

            # Store in memory
            self.memory.add_exchange(
                question,
                result['answer'],
                {'sources': search_results, 'confidence': result['score']}
            )

            return response

        except Exception as e:
            return {
                'answer': f"Error processing question: {str(e)}",
                'confidence': 0.0,
                'sources': search_results,
                'query_time': time.time() - start_time
            }

    def _format_conversation_context(self, recent_context: List[Dict], relevant_context: List[Dict]) -> str:
        """Format conversation context for prompt"""
        context_parts = []

        if recent_context:
            context_parts.append("Recent conversation:")
            for exchange in recent_context[-2:]:
                context_parts.append(f"Q: {exchange['question']}")
                context_parts.append(f"A: {exchange['answer'][:150]}...")

        if relevant_context and relevant_context != recent_context:
            context_parts.append("\nRelevant previous discussion:")
            for exchange in relevant_context[:1]:  # Just 1 to save space
                if exchange not in recent_context:
                    context_parts.append(f"Q: {exchange['question']}")
                    context_parts.append(f"A: {exchange['answer'][:100]}...")

        return "\n".join(context_parts)

# Create a file called app.py for Streamlit interface
app_code = '''
import streamlit as st
import os
from studymate import StudyMate  # Import your StudyMate class

st.set_page_config(page_title="StudyMate", page_icon="üéì", layout="wide")

def main():
    st.title("üéì StudyMate - AI-Powered PDF Q&A")
    st.markdown("Upload your PDF documents and ask questions!")

    # Initialize StudyMate
    if 'studymate' not in st.session_state:
        st.session_state.studymate = StudyMate()
        st.session_state.messages = []

    # Sidebar for PDF upload
    with st.sidebar:
        st.header("üìÅ Upload Documents")
        uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")

        if uploaded_file is not None:
            # Save uploaded file temporarily
            with open("temp_document.pdf", "wb") as f:
                f.write(uploaded_file.getbuffer())

            # Process PDF
            with st.spinner("Processing PDF..."):
                chunks = st.session_state.studymate.upload_and_process_pdf("temp_document.pdf")

            st.success(f"‚úÖ Processed {chunks} text chunks")
            os.remove("temp_document.pdf")  # Clean up

    # Main chat interface
    st.header("üí¨ Ask Questions")

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input
    if prompt := st.chat_input("Ask a question about your document"):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Get AI response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = st.session_state.studymate.ask_question(prompt)

                st.markdown(response['answer'])

                # Show additional info
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Confidence", f"{response['confidence']:.2f}")
                with col2:
                    st.metric("Query Time", f"{response['query_time']:.2f}s")
                with col3:
                    st.metric("Sources", len(response['sources']))

        # Add assistant response
        st.session_state.messages.append({"role": "assistant", "content": response['answer']})

if __name__ == "__main__":
    main()
'''

# Save the Streamlit app
with open('app.py', 'w') as f:
    f.write(app_code)

print("‚úÖ Streamlit app created as 'app.py'")

import subprocess
import threading
import time

# Run Streamlit in background
def run_streamlit():
    subprocess.run(["streamlit", "run", "app.py", "--server.port", "8501", "--server.address", "0.0.0.0"])

# Start Streamlit
thread = threading.Thread(target=run_streamlit)
thread.daemon = True
thread.start()

# Wait for Streamlit to start
time.sleep(10)

# Use Colab's built-in port forwarding
print("üåê Click on the URL that appears above to access your Streamlit app")
print("Look for a message like 'External URL: https://xyz-8501.preview.app.googleusercontent.com'")

# Quick test
def test_studymate():
    studymate = StudyMate()

    # Test without PDF
    response = studymate.ask_question("What is machine learning?")
    print("Test 1 - No PDF:", response['answer'])

    # You can add more tests here
    print("‚úÖ Basic tests completed!")

test_studymate()